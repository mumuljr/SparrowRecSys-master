import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from sklearn.decomposition import TruncatedSVD
from scipy.sparse import csr_matrix
import logging
import os
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

OUTPUT_DIR = "../src/main/resources/webroot/modeldata"
DATASET_PATH = "ml-25m/ratings.csv"

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ============================================================
#                  Item2Vec è®­ç»ƒéƒ¨åˆ†ï¼ˆä¿ç•™åŸæ ·ï¼‰
# ============================================================

def train_item2vec_large_dataset():
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¤§æ•°æ®é›† Item2Vec æ¨¡å‹...")

    # è¯»å–æ•°æ®
    print("ğŸ“– è¯»å–è¯„åˆ†æ•°æ®...")
    if not os.path.exists(DATASET_PATH):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶: {DATASET_PATH}")

    ratings = pd.read_csv(DATASET_PATH)
    print(f"æ€»è¯„åˆ†æ•°: {len(ratings):,}")

    # ç­›é€‰æ´»è·ƒç”¨æˆ·ä¸çƒ­é—¨ç”µå½±
    print("ğŸ”„ æ•°æ®é¢„å¤„ç†...")
    user_counts = ratings["userId"].value_counts()
    movie_counts = ratings["movieId"].value_counts()

    active_users = user_counts[user_counts >= 20].index
    popular_movies = movie_counts[movie_counts >= 50].index

    filtered_ratings = ratings[
        (ratings["userId"].isin(active_users)) &
        (ratings["movieId"].isin(popular_movies))
        ].copy()

    print(f"ç­›é€‰åè¯„åˆ†æ•°: {len(filtered_ratings):,}")

    # åˆ›å»ºåºåˆ—
    print("ğŸ“ åˆ›å»ºç”¨æˆ·è§‚å½±åºåˆ—...")
    filtered_ratings = filtered_ratings.sort_values(["userId", "timestamp"])

    user_sequences = []
    for user_id in tqdm(filtered_ratings["userId"].unique()):
        seq = filtered_ratings[filtered_ratings["userId"] == user_id]["movieId"].astype(str).tolist()
        if len(seq) >= 5:
            user_sequences.append(seq)

    print(f"åˆ›å»ºäº† {len(user_sequences):,} ä¸ªç”µå½±åºåˆ—")

    # è®­ç»ƒ Word2Vec
    print("ğŸ¤– å¼€å§‹è®­ç»ƒ Word2Vec ...")
    model = Word2Vec(
        sentences=user_sequences,
        vector_size=128,
        window=5,
        min_count=5,
        workers=4,
        epochs=10,
        sg=1
    )

    # ä¿å­˜ç”µå½± embedding
    item2vec_path = os.path.join(OUTPUT_DIR, "item2vecEmb_large.csv")
    print(f"ğŸ’¾ ä¿å­˜ç”µå½± embedding åˆ°: {item2vec_path}")

    with open(item2vec_path, "w") as f:
        for movie_id in model.wv.index_to_key:
            emb = " ".join(map(str, model.wv[movie_id]))
            f.write(f"{movie_id}:{emb}\n")

    return model, filtered_ratings


# ============================================================
#       â­  æ”¹è¿›ï¼šä½¿ç”¨ç¨€ç–çŸ©é˜µ + SVDï¼ˆä¸ä¼šçˆ†å†…å­˜ï¼‰ â­
# ============================================================

def train_user_embeddings_large_dataset(filtered_ratings):
    print("ğŸš€ ä½¿ç”¨ç¨€ç–çŸ©é˜µè®­ç»ƒç”¨æˆ· Embeddingï¼ˆSparse SVDï¼‰...")

    # è·å–å”¯ä¸€çš„ç”¨æˆ·ä¸ç”µå½±
    users = filtered_ratings["userId"].unique()
    movies = filtered_ratings["movieId"].unique()

    print(f"ç”¨æˆ·æ•°: {len(users)}, ç”µå½±æ•°: {len(movies)}")

    user_to_index = {u: i for i, u in enumerate(users)}
    movie_to_index = {m: i for i, m in enumerate(movies)}

    rows, cols, data = [], [], []

    print("ğŸ“¦ æ„å»º CSR ç¨€ç–çŸ©é˜µï¼ˆä¸ä¼šçˆ†å†…å­˜ï¼‰...")
    for row in tqdm(filtered_ratings.itertuples(), total=len(filtered_ratings)):
        rows.append(user_to_index[row.userId])
        cols.append(movie_to_index[row.movieId])
        data.append(row.rating)

    # æ„å»ºç¨€ç–ç”¨æˆ·-ç”µå½±çŸ©é˜µ
    matrix = csr_matrix((data, (rows, cols)),
                        shape=(len(users), len(movies)))

    print("âœ” ç¨€ç–çŸ©é˜µæ„å»ºå®Œæˆï¼å¤§å°ï¼š", matrix.shape)

    # SVD é™ç»´
    print("ğŸ” å¼€å§‹ SVD é™ç»´ (128 ç»´)...")
    svd = TruncatedSVD(n_components=128, random_state=42)
    user_embeddings = svd.fit_transform(matrix)

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(OUTPUT_DIR, "userEmb_large.csv")
    print(f"ğŸ’¾ ä¿å­˜ç”¨æˆ· embedding åˆ°: {output_path}")

    with open(output_path, "w") as f:
        for i, user_id in enumerate(users):
            emb = " ".join(map(str, user_embeddings[i]))
            f.write(f"{user_id}:{emb}\n")

    print(f"ğŸ‰ ä¿å­˜äº† {len(users)} ä¸ªç”¨æˆ· embedding")


# ============================================================
#                           ä¸»ç¨‹åº
# ============================================================

def main():
    print("=" * 60)
    print("ğŸ¬ SparrowRecSys å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå¯åŠ¨")
    print("=" * 60)

    model, filtered = train_item2vec_large_dataset()
    train_user_embeddings_large_dataset(filtered)

    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
