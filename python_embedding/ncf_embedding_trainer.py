import pandas as pd
import numpy as np
import logging
import sys
import os
import tensorflow as tf
from tensorflow.keras import layers, Model, optimizers, losses, metrics
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle

# è®¾ç½®æ—¥å¿—
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
np.random.seed(42)
tf.random.set_seed(42)

# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    try:
        tf.config.experimental.set_memory_growth(physical_devices[0], True)
        print(f"âœ… GPUå¯ç”¨: {physical_devices}")
    except Exception as e:
        print(f"âš ï¸ GPUè®¾ç½®è­¦å‘Š: {e}")
else:
    print("â„¹ï¸ ä½¿ç”¨CPUè®­ç»ƒ")


class NCFModel:
    """NCFæ¨¡å‹å®ç°ç±»ï¼ˆNeural Collaborative Filteringï¼‰"""

    def __init__(self, num_users, num_items, embedding_dim=128, dropout_rate=0.2):
        """
        åˆå§‹åŒ–NCFæ¨¡å‹
        :param num_users: ç”¨æˆ·æ•°é‡
        :param num_items: ç‰©å“æ•°é‡
        :param embedding_dim: åµŒå…¥ç»´åº¦
        :param dropout_rate: Dropoutç‡
        """
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        self.dropout_rate = dropout_rate
        self.model = self._build_model()

    def _build_mlp_part(self, user_input, item_input):
        """æ„å»ºMLPéƒ¨åˆ†ç½‘ç»œ"""
        # MLPåµŒå…¥å±‚
        user_mlp_embedding = layers.Embedding(
            input_dim=self.num_users,
            output_dim=self.embedding_dim,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6),
            name='user_mlp_embedding'
        )(user_input)
        item_mlp_embedding = layers.Embedding(
            input_dim=self.num_items,
            output_dim=self.embedding_dim,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6),
            name='item_mlp_embedding'
        )(item_input)

        # å±•å¹³
        user_flat = layers.Flatten()(user_mlp_embedding)
        item_flat = layers.Flatten()(item_mlp_embedding)

        # æ‹¼æ¥
        concat = layers.Concatenate()([user_flat, item_flat])

        # MLPå±‚
        mlp = layers.Dense(512, activation='relu', kernel_initializer='he_normal')(concat)
        mlp = layers.Dropout(self.dropout_rate)(mlp)
        mlp = layers.BatchNormalization()(mlp)

        mlp = layers.Dense(256, activation='relu', kernel_initializer='he_normal')(mlp)
        mlp = layers.Dropout(self.dropout_rate)(mlp)
        mlp = layers.BatchNormalization()(mlp)

        mlp = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(mlp)
        mlp = layers.Dropout(self.dropout_rate)(mlp)
        mlp = layers.BatchNormalization()(mlp)

        return mlp

    def _build_gmf_part(self, user_input, item_input):
        """æ„å»ºGMFï¼ˆGeneralized Matrix Factorizationï¼‰éƒ¨åˆ†ç½‘ç»œ"""
        # GMFåµŒå…¥å±‚
        user_gmf_embedding = layers.Embedding(
            input_dim=self.num_users,
            output_dim=self.embedding_dim,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6),
            name='user_gmf_embedding'
        )(user_input)
        item_gmf_embedding = layers.Embedding(
            input_dim=self.num_items,
            output_dim=self.embedding_dim,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6),
            name='item_gmf_embedding'
        )(item_input)

        # å…ƒç´ çº§ä¹˜æ³•
        gmf = layers.Multiply()([user_gmf_embedding, item_gmf_embedding])
        gmf = layers.Flatten()(gmf)

        return gmf

    def _build_model(self):
        """æ„å»ºå®Œæ•´çš„NCFæ¨¡å‹ï¼ˆGMF + MLPï¼‰"""
        # è¾“å…¥å±‚
        user_input = layers.Input(shape=(1,), name='user_input')
        item_input = layers.Input(shape=(1,), name='item_input')

        # æ„å»ºGMFå’ŒMLPéƒ¨åˆ†
        gmf_output = self._build_gmf_part(user_input, item_input)
        mlp_output = self._build_mlp_part(user_input, item_input)

        # æ‹¼æ¥GMFå’ŒMLPè¾“å‡º
        concat = layers.Concatenate()([gmf_output, mlp_output])

        # è¾“å‡ºå±‚
        output = layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(concat)

        # æ„å»ºæ¨¡å‹
        model = Model(inputs=[user_input, item_input], outputs=output)

        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss=losses.BinaryCrossentropy(),
            metrics=[
                metrics.AUC(name='auc'),
                metrics.BinaryAccuracy(name='accuracy')
            ]
        )

        return model

    def train(self, train_data, val_data, epochs=20, batch_size=2048):
        """
        è®­ç»ƒæ¨¡å‹
        :param train_data: è®­ç»ƒæ•°æ® (users, items, labels)
        :param val_data: éªŒè¯æ•°æ® (users, items, labels)
        :param epochs: è®­ç»ƒè½®æ•°
        :param batch_size: æ‰¹æ¬¡å¤§å°
        """
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_users, train_items, train_labels = train_data
        val_users, val_items, val_labels = val_data

        # å›è°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(
                monitor='val_auc',
                patience=3,
                mode='max',
                restore_best_weights=True
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=2,
                min_lr=1e-6
            ),
            ModelCheckpoint(
                'ncf_model_best.h5',
                monitor='val_auc',
                save_best_only=True,
                mode='max'
            )
        ]

        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            x=[train_users, train_items],
            y=train_labels,
            validation_data=([val_users, val_items], val_labels),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            shuffle=True,
            verbose=1
        )

        return history

    def save_model(self, path='../src/main/resources/webroot/modeldata/ncf_model'):
        """ä¿å­˜æ¨¡å‹"""
        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(path), exist_ok=True)

        # ä¿å­˜å®Œæ•´æ¨¡å‹
        self.model.save(f'{path}.h5')
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {path}.h5")

        # ä¿å­˜æ¨¡å‹é…ç½®
        config = {
            'num_users': self.num_users,
            'num_items': self.num_items,
            'embedding_dim': self.embedding_dim,
            'dropout_rate': self.dropout_rate
        }
        np.save(f'{path}_config.npy', config)
        print(f"âœ… æ¨¡å‹é…ç½®å·²ä¿å­˜è‡³: {path}_config.npy")

    def load_embeddings(self):
        """æå–å¹¶è¿”å›ç”¨æˆ·å’Œç‰©å“åµŒå…¥å‘é‡"""
        # è·å–åµŒå…¥å±‚æƒé‡
        user_gmf_emb = self.model.get_layer('user_gmf_embedding').get_weights()[0]
        item_gmf_emb = self.model.get_layer('item_gmf_embedding').get_weights()[0]
        user_mlp_emb = self.model.get_layer('user_mlp_embedding').get_weights()[0]
        item_mlp_emb = self.model.get_layer('item_mlp_embedding').get_weights()[0]

        # èåˆGMFå’ŒMLPåµŒå…¥
        user_embeddings = np.concatenate([user_gmf_emb, user_mlp_emb], axis=1)
        item_embeddings = np.concatenate([item_gmf_emb, item_mlp_emb], axis=1)

        return user_embeddings, item_embeddings


def prepare_ncf_data():
    """å‡†å¤‡NCFè®­ç»ƒæ•°æ®ï¼ˆå¤„ç†ml-25mæ•°æ®é›†ï¼‰"""
    print("ğŸ“– è¯»å–è¯„åˆ†æ•°æ®...")
    ratings = pd.read_csv('ml-25m/ratings.csv')
    print(f"æ€»è¯„åˆ†æ•°: {len(ratings):,}")
    print(f"ç”¨æˆ·æ•°: {ratings['userId'].nunique():,}")
    print(f"ç”µå½±æ•°: {ratings['movieId'].nunique():,}")

    # æ•°æ®é¢„å¤„ç† - ç­›é€‰æ´»è·ƒç”¨æˆ·å’Œçƒ­é—¨ç”µå½±
    print("ğŸ”„ æ•°æ®é¢„å¤„ç†...")

    # ç”¨æˆ·è¯„åˆ†ç»Ÿè®¡
    user_counts = ratings['userId'].value_counts()
    movie_counts = ratings['movieId'].value_counts()

    # ç­›é€‰æ ‡å‡†: ç”¨æˆ·è‡³å°‘20ä¸ªè¯„åˆ†ï¼Œç”µå½±è‡³å°‘50ä¸ªè¯„åˆ†
    active_users = user_counts[user_counts >= 20].index
    popular_movies = movie_counts[movie_counts >= 50].index

    # ç­›é€‰æ•°æ®
    filtered_ratings = ratings[
        (ratings['userId'].isin(active_users)) &
        (ratings['movieId'].isin(popular_movies))
        ].copy()

    print(f"ç­›é€‰åè¯„åˆ†æ•°: {len(filtered_ratings):,}")
    print(f"ç­›é€‰åç”¨æˆ·æ•°: {filtered_ratings['userId'].nunique():,}")
    print(f"ç­›é€‰åç”µå½±æ•°: {filtered_ratings['movieId'].nunique():,}")

    # æ ‡ç­¾ç¼–ç ï¼ˆå°†ç”¨æˆ·IDå’Œç”µå½±IDè½¬æ¢ä¸ºè¿ç»­ç´¢å¼•ï¼‰
    print("ğŸ”¢ æ ‡ç­¾ç¼–ç ç”¨æˆ·å’Œç‰©å“ID...")
    user_encoder = LabelEncoder()
    item_encoder = LabelEncoder()

    filtered_ratings['user_idx'] = user_encoder.fit_transform(filtered_ratings['userId'])
    filtered_ratings['item_idx'] = item_encoder.fit_transform(filtered_ratings['movieId'])

    # å°†è¯„åˆ†è½¬æ¢ä¸ºéšå¼åé¦ˆï¼ˆNCFé€šå¸¸ç”¨äºæ¨èï¼Œä½¿ç”¨äºŒåˆ†ç±»ï¼‰
    # è¯„åˆ†>=3è§†ä¸ºæ­£æ ·æœ¬ï¼Œå¦åˆ™ä¸ºè´Ÿæ ·æœ¬
    filtered_ratings['label'] = (filtered_ratings['rating'] >= 3).astype(int)

    # ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬æ•°é‡
    positive_samples = filtered_ratings[filtered_ratings['label'] == 1]
    negative_samples = filtered_ratings[filtered_ratings['label'] == 0]

    pos_count = len(positive_samples)
    neg_count = len(negative_samples)

    print(f"\nğŸ“Š æ ·æœ¬åˆ†å¸ƒ:")
    print(f"æ­£æ ·æœ¬æ•°é‡: {pos_count:,}")
    print(f"è´Ÿæ ·æœ¬æ•°é‡: {neg_count:,}")
    print(f"æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 1:{neg_count / pos_count:.2f}")

    # æ”¹è¿›çš„è´Ÿé‡‡æ ·é€»è¾‘ - å¤„ç†è´Ÿæ ·æœ¬ä¸è¶³çš„æƒ…å†µ
    print("ğŸ² è´Ÿé‡‡æ ·å¤„ç†...")
    if neg_count >= pos_count:
        # è´Ÿæ ·æœ¬å……è¶³ï¼Œé‡‡æ ·ä¸æ­£æ ·æœ¬æ•°é‡ç›¸åŒçš„è´Ÿæ ·æœ¬
        negative_samples = negative_samples.sample(
            n=pos_count,
            random_state=42,
            replace=False
        )
        print(f"âœ… è´Ÿæ ·æœ¬å……è¶³ï¼Œé‡‡æ · {pos_count:,} ä¸ªè´Ÿæ ·æœ¬")
    else:
        # è´Ÿæ ·æœ¬ä¸è¶³ï¼Œæœ‰ä¸¤ç§å¤„ç†æ–¹å¼ï¼š
        # 1. ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬ï¼ŒåŒæ—¶å¯¹æ­£æ ·æœ¬ä¸‹é‡‡æ ·
        print(f"âš ï¸ è´Ÿæ ·æœ¬ä¸è¶³ï¼ˆ{neg_count} < {pos_count}ï¼‰ï¼Œè°ƒæ•´æ ·æœ¬æ•°é‡")
        # å¯¹æ­£æ ·æœ¬ä¸‹é‡‡æ ·åˆ°è´Ÿæ ·æœ¬æ•°é‡
        positive_samples = positive_samples.sample(
            n=neg_count,
            random_state=42,
            replace=False
        )
        print(f"âœ… æ­£æ ·æœ¬ä¸‹é‡‡æ ·è‡³ {neg_count:,} ä¸ª")

    # åˆå¹¶æ­£è´Ÿæ ·æœ¬
    train_data = pd.concat([positive_samples, negative_samples])
    train_data = shuffle(train_data, random_state=42)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    print("ğŸ“Š åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
    train_df, val_df = train_test_split(
        train_data,
        test_size=0.1,
        random_state=42,
        stratify=train_data['label']  # åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
    )

    print(
        f"è®­ç»ƒé›†å¤§å°: {len(train_df):,} (æ­£æ ·æœ¬: {train_df['label'].sum()}, è´Ÿæ ·æœ¬: {len(train_df) - train_df['label'].sum()})")
    print(
        f"éªŒè¯é›†å¤§å°: {len(val_df):,} (æ­£æ ·æœ¬: {val_df['label'].sum()}, è´Ÿæ ·æœ¬: {len(val_df) - val_df['label'].sum()})")

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_users = train_df['user_idx'].values
    train_items = train_df['item_idx'].values
    train_labels = train_df['label'].values

    # å‡†å¤‡éªŒè¯æ•°æ®
    val_users = val_df['user_idx'].values
    val_items = val_df['item_idx'].values
    val_labels = val_df['label'].values

    # è·å–ç¼–ç åçš„ç”¨æˆ·å’Œç‰©å“æ•°é‡
    num_users = len(user_encoder.classes_)
    num_items = len(item_encoder.classes_)

    # ä¿å­˜ç¼–ç å™¨æ˜ å°„å…³ç³»
    print("ğŸ’¾ ä¿å­˜ç”¨æˆ·å’Œç‰©å“IDæ˜ å°„...")
    # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs('../src/main/resources/webroot/modeldata/', exist_ok=True)

    user_mapping = dict(zip(user_encoder.classes_, range(num_users)))
    item_mapping = dict(zip(item_encoder.classes_, range(num_items)))

    # ä¿å­˜æ˜ å°„å…³ç³»
    np.save('../src/main/resources/webroot/modeldata/user_mapping.npy', user_mapping)
    np.save('../src/main/resources/webroot/modeldata/item_mapping.npy', item_mapping)

    return (train_users, train_items, train_labels), \
        (val_users, val_items, val_labels), \
        num_users, num_items, user_encoder, item_encoder


def save_ncf_embeddings(ncf_model, user_encoder, item_encoder):
    """ä¿å­˜NCFç”Ÿæˆçš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥å‘é‡"""
    print("ğŸ’¾ æå–å¹¶ä¿å­˜NCFåµŒå…¥å‘é‡...")

    # è·å–åµŒå…¥å‘é‡
    user_embeddings, item_embeddings = ncf_model.load_embeddings()

    # ä¿å­˜ç”¨æˆ·embeddings
    user_emb_list = []
    for idx, original_user_id in enumerate(user_encoder.classes_):
        embedding = ' '.join(map(str, user_embeddings[idx]))
        user_emb_list.append(f"{original_user_id}:{embedding}")

    # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs('../src/main/resources/webroot/modeldata/', exist_ok=True)

    with open('../src/main/resources/webroot/modeldata/ncf_userEmb_large.csv', 'w', encoding='utf-8') as f:
        f.write('\n'.join(user_emb_list))

    # ä¿å­˜ç‰©å“embeddings
    item_emb_list = []
    for idx, original_item_id in enumerate(item_encoder.classes_):
        embedding = ' '.join(map(str, item_embeddings[idx]))
        item_emb_list.append(f"{original_item_id}:{embedding}")

    with open('../src/main/resources/webroot/modeldata/ncf_itemEmb_large.csv', 'w', encoding='utf-8') as f:
        f.write('\n'.join(item_emb_list))

    print(f"âœ… ä¿å­˜äº† {len(user_emb_list):,} ä¸ªç”¨æˆ·embeddings")
    print(f"âœ… ä¿å­˜äº† {len(item_emb_list):,} ä¸ªç‰©å“embeddings")


def main():
    print("=" * 60)
    print("ğŸ¬ SparrowRecSys NCFæ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    try:
        # 1. å‡†å¤‡æ•°æ®
        train_data, val_data, num_users, num_items, user_encoder, item_encoder = prepare_ncf_data()

        # 2. åˆ›å»ºNCFæ¨¡å‹
        print("ğŸ¤– åˆ›å»ºNCFæ¨¡å‹...")
        ncf_model = NCFModel(
            num_users=num_users,
            num_items=num_items,
            embedding_dim=128,
            dropout_rate=0.2
        )

        # æ‰“å°æ¨¡å‹ç»“æ„
        print("\nğŸ“‹ æ¨¡å‹ç»“æ„:")
        ncf_model.model.summary()

        # 3. è®­ç»ƒæ¨¡å‹
        print("\nğŸš€ å¼€å§‹è®­ç»ƒNCFæ¨¡å‹...")
        history = ncf_model.train(
            train_data=train_data,
            val_data=val_data,
            epochs=20,
            batch_size=2048
        )

        # 4. ä¿å­˜æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜NCFæ¨¡å‹...")
        ncf_model.save_model()

        # 5. æå–å¹¶ä¿å­˜åµŒå…¥å‘é‡
        save_ncf_embeddings(ncf_model, user_encoder, item_encoder)

        # 6. æ‰“å°è®­ç»ƒç»“æœ
        print("\nğŸ“Š è®­ç»ƒç»“æœ:")
        print(f"æœ€ä½³éªŒè¯AUC: {max(history.history['val_auc']):.4f}")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history.history['val_accuracy']):.4f}")

        print("\nğŸ‰ NCFæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - ncf_model.h5: NCFå®Œæ•´æ¨¡å‹")
        print("  - ncf_model_config.npy: æ¨¡å‹é…ç½®å‚æ•°")
        print("  - ncf_userEmb_large.csv: NCFç”¨æˆ·embeddings")
        print("  - ncf_itemEmb_large.csv: NCFç‰©å“embeddings")
        print("  - user_mapping.npy: ç”¨æˆ·IDæ˜ å°„å…³ç³»")
        print("  - item_mapping.npy: ç‰©å“IDæ˜ å°„å…³ç³»")

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()